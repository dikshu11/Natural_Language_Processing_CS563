{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1701CS19_assignment05.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "nlp",
      "language": "python",
      "name": "nlp"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Hdp1S1cWhn7"
      },
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "from string import digits\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.layers import Input, LSTM, Embedding, Dense, TimeDistributed, Flatten, Dropout, Bidirectional\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Model, Sequential, load_model\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from numpy import array, argmax\n",
        "from numpy.random import rand, shuffle\n",
        "from nltk.translate.bleu_score import corpus_bleu, sentence_bleu\n",
        "import scipy\n",
        "import statsmodels\n",
        "import sklearn\n",
        "import tensorflow\n",
        "import keras\n",
        "from io import open\n",
        "import unicodedata\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "import os\n",
        "import time\n",
        "import re\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "from unicodedata import normalize\n",
        "from numpy import array\n",
        "from pickle import dump, load\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.layers import RepeatVector, TimeDistributed\n",
        "from keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aApRwSzKWmtV"
      },
      "source": [
        "MAX_LENGTH = 100\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8e9H0SX-WpBe",
        "outputId": "9c85acbd-affc-497c-84f6-62947a206194"
      },
      "source": [
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1iljWhk1LVNEdXOYyfxCZd_Osv9D5TJ76' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1iljWhk1LVNEdXOYyfxCZd_Osv9D5TJ76\" -O assign4dataset.zip && rm -rf /tmp/cookies.txt\n",
        "! unzip assign4dataset.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-17 15:49:58--  https://docs.google.com/uc?export=download&confirm=&id=1iljWhk1LVNEdXOYyfxCZd_Osv9D5TJ76\n",
            "Resolving docs.google.com (docs.google.com)... 172.217.164.174, 2607:f8b0:4004:815::200e\n",
            "Connecting to docs.google.com (docs.google.com)|172.217.164.174|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0g-1k-docs.googleusercontent.com/docs/securesc/v645tahvv65b74ld578qjta4nm7j1l8l/3q0sojiov4c74t05h5j3ig3s9gmlk86q/1618674600000/15072518060054569691/08413793949905811032Z/1iljWhk1LVNEdXOYyfxCZd_Osv9D5TJ76?e=download [following]\n",
            "--2021-04-17 15:50:01--  https://doc-0g-1k-docs.googleusercontent.com/docs/securesc/v645tahvv65b74ld578qjta4nm7j1l8l/3q0sojiov4c74t05h5j3ig3s9gmlk86q/1618674600000/15072518060054569691/08413793949905811032Z/1iljWhk1LVNEdXOYyfxCZd_Osv9D5TJ76?e=download\n",
            "Resolving doc-0g-1k-docs.googleusercontent.com (doc-0g-1k-docs.googleusercontent.com)... 142.250.73.193, 2607:f8b0:4004:829::2001\n",
            "Connecting to doc-0g-1k-docs.googleusercontent.com (doc-0g-1k-docs.googleusercontent.com)|142.250.73.193|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://docs.google.com/nonceSigner?nonce=un6kiijqr7hbg&continue=https://doc-0g-1k-docs.googleusercontent.com/docs/securesc/v645tahvv65b74ld578qjta4nm7j1l8l/3q0sojiov4c74t05h5j3ig3s9gmlk86q/1618674600000/15072518060054569691/08413793949905811032Z/1iljWhk1LVNEdXOYyfxCZd_Osv9D5TJ76?e%3Ddownload&hash=kgku6sfvjblahv4jrq04n876t6sf0q5k [following]\n",
            "--2021-04-17 15:50:01--  https://docs.google.com/nonceSigner?nonce=un6kiijqr7hbg&continue=https://doc-0g-1k-docs.googleusercontent.com/docs/securesc/v645tahvv65b74ld578qjta4nm7j1l8l/3q0sojiov4c74t05h5j3ig3s9gmlk86q/1618674600000/15072518060054569691/08413793949905811032Z/1iljWhk1LVNEdXOYyfxCZd_Osv9D5TJ76?e%3Ddownload&hash=kgku6sfvjblahv4jrq04n876t6sf0q5k\n",
            "Connecting to docs.google.com (docs.google.com)|172.217.164.174|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://doc-0g-1k-docs.googleusercontent.com/docs/securesc/v645tahvv65b74ld578qjta4nm7j1l8l/3q0sojiov4c74t05h5j3ig3s9gmlk86q/1618674600000/15072518060054569691/08413793949905811032Z/1iljWhk1LVNEdXOYyfxCZd_Osv9D5TJ76?e=download&nonce=un6kiijqr7hbg&user=08413793949905811032Z&hash=mb9p4c0a7que99aqeliqllg07a9mvtc5 [following]\n",
            "--2021-04-17 15:50:01--  https://doc-0g-1k-docs.googleusercontent.com/docs/securesc/v645tahvv65b74ld578qjta4nm7j1l8l/3q0sojiov4c74t05h5j3ig3s9gmlk86q/1618674600000/15072518060054569691/08413793949905811032Z/1iljWhk1LVNEdXOYyfxCZd_Osv9D5TJ76?e=download&nonce=un6kiijqr7hbg&user=08413793949905811032Z&hash=mb9p4c0a7que99aqeliqllg07a9mvtc5\n",
            "Connecting to doc-0g-1k-docs.googleusercontent.com (doc-0g-1k-docs.googleusercontent.com)|142.250.73.193|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 889658 (869K) [application/x-zip-compressed]\n",
            "Saving to: ‘assign4dataset.zip’\n",
            "\n",
            "assign4dataset.zip  100%[===================>] 868.81K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2021-04-17 15:50:01 (31.5 MB/s) - ‘assign4dataset.zip’ saved [889658/889658]\n",
            "\n",
            "Archive:  assign4dataset.zip\n",
            "  inflating: english.train.txt       \n",
            "  inflating: hindi.test.txt          \n",
            "  inflating: hindi.train.txt         \n",
            "  inflating: english.test.txt        \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kfq3METaWsbI"
      },
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {\"sos\":0, \"eos\": 1}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"sos\", 1: \"eos\"}\n",
        "        self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1yqsiMCdj9G"
      },
      "source": [
        "def load_doc(filename):\n",
        "    file = open(filename, mode='rt', encoding='utf-8')\n",
        "    text = file.read()\n",
        "    file.close()\n",
        "    return text\n",
        "\n",
        "def to_pairs(english_text, hindi_text):\n",
        "    english_lines = english_text.strip().split('\\n')\n",
        "    hindi_lines = hindi_text.strip().split('\\n')\n",
        "    pairs = []\n",
        "    for i in range(len(hindi_lines)):\n",
        "        pairs.append([])\n",
        "        pairs[i].append(pre_process_english_sentence(english_lines[i]))\n",
        "        pairs[i].append(pre_process_hindi_sentence(hindi_lines[i]))\n",
        "    return pairs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-JwzF6-dkib"
      },
      "source": [
        "def clean_text(text):\n",
        "    text = text.replace(u',','')\n",
        "    text = text.replace(u'\"','')\n",
        "    text = text.replace(u'\"','')\n",
        "    text = text.replace(u\"‘‘\",'')\n",
        "    text = text.replace(u\"’’\",'')\n",
        "    text = text.replace(u\"''\",'')\n",
        "    text = text.replace(u\"।\",'')\n",
        "    text=text.replace(u',','')\n",
        "    text=text.replace(u'\"','')\n",
        "    text=text.replace(u'(','')\n",
        "    text=text.replace(u')','')\n",
        "    text=text.replace(u'\"','')\n",
        "    text=text.replace(u':','')\n",
        "    text=text.replace(u\"'\",'')\n",
        "    text=text.replace(u\"‘‘\",'')\n",
        "    text=text.replace(u\"’’\",'')\n",
        "    text=text.replace(u\"''\",'')\n",
        "    text=text.replace(u\".\",'')\n",
        "    text=text.replace(u\"-\",'')\n",
        "    text=text.replace(u\"।\",'')\n",
        "    text=text.replace(u\"?\",'')\n",
        "    text=text.replace(u\"\\\\\",'')\n",
        "    text=text.replace(u\"_\",'')\n",
        "    text=text.replace(\"'\", \"\")\n",
        "    text=text.replace('\"', \"\")\n",
        "    text= re.sub(\"'\", '', text)\n",
        "    text= re.sub(\"’\", '', text)\n",
        "    text=re.sub('[0-9+\\-*/.%]', '', text)\n",
        "    text=text.strip()\n",
        "    text=re.sub(' +', ' ',text)\n",
        "    exclude = set(string.punctuation)\n",
        "    text= ''.join(ch for ch in text if ch not in exclude)\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxN7Kq2DdmS7"
      },
      "source": [
        "def pre_process_english_sentence(line):\n",
        "    line = line.lower()\n",
        "    line = clean_text(line)\n",
        "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
        "    line = normalize('NFD', line).encode('ascii', 'ignore')\n",
        "    line = line.decode('UTF-8')\n",
        "    line = line.split()\n",
        "    line = [re_print.sub('', w) for w in line]\n",
        "    line = [word for word in line if word.isalpha()]\n",
        "    line = ' '.join(line)\n",
        "    return line\n",
        "\n",
        "def pre_process_hindi_sentence(line):\n",
        "    line=re.sub('[a-zA-Z]', '', line)\n",
        "    line = clean_text(line)\n",
        "    exclude = set(string.punctuation)\n",
        "    remove_digits = str.maketrans('', '', string.digits)\n",
        "    line = re.sub(\"'\", '', line)\n",
        "    line = ''.join(ch for ch in line if ch not in exclude)\n",
        "    line = line.translate(remove_digits)\n",
        "    line = re.sub(\"[२३०८१५७९४६]\", \"\", line)\n",
        "    line = line.strip()\n",
        "    line = re.sub(\" +\", \" \", line)\n",
        "    return (line)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZPl9mbydqjm"
      },
      "source": [
        "def prepareData(pairs):\n",
        "    english_lang = Lang('eng')\n",
        "    hindi_lang = Lang('hin')\n",
        "    for pair in pairs:\n",
        "        english_lang.addSentence(pair[0])\n",
        "        hindi_lang.addSentence(pair[1])\n",
        "    return english_lang, hindi_lang"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSzx9lrrdtgv"
      },
      "source": [
        "train_eng_text = load_doc('english.train.txt')\n",
        "train_hindi_text = load_doc('hindi.train.txt')\n",
        "train_pair = to_pairs(train_eng_text, train_hindi_text)\n",
        "english_lang, hindi_lang = prepareData(train_pair)\n",
        "\n",
        "train_df = pd.DataFrame(train_pair)\n",
        "train_df.columns = [\"english\", \"hindi\"]\n",
        "\n",
        "list_len = []\n",
        "for l in train_df.english:\n",
        "    list_len.append(len(l.split(' ')))\n",
        "max_len_eng = np.max(list_len)\n",
        "\n",
        "list_len = []\n",
        "for l in train_df.hindi:\n",
        "    list_len.append(len(l.split(' ')))\n",
        "max_len_hindi = np.max(list_len)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "jOxbCUKUdvjo",
        "outputId": "721d4f45-729b-4872-bf18-f006187661c6"
      },
      "source": [
        "train_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>english</th>\n",
              "      <th>hindi</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>give your application an accessibility workout</td>\n",
              "      <td>अपने अनुप्रयोग को पहुंचनीयता व्यायाम का लाभ दें</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>accerciser accessibility explorer</td>\n",
              "      <td>एक्सेर्साइसर पहुंचनीयता अन्वेषक</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>the default plugin layout for the bottom panel</td>\n",
              "      <td>निचले पटल के लिए डिफोल्ट प्लगइन खाका</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>the default plugin layout for the top panel</td>\n",
              "      <td>ऊपरी पटल के लिए डिफोल्ट प्लगइन खाका</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>a list of plugins that are disabled by default</td>\n",
              "      <td>उन प्लगइनों की सूची जिन्हें डिफोल्ट रूप से निष...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49995</th>\n",
              "      <td>audio test</td>\n",
              "      <td>ऑडियो जाँच</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49996</th>\n",
              "      <td>silent</td>\n",
              "      <td>मूक</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49997</th>\n",
              "      <td>video test</td>\n",
              "      <td>वीडियो जाँच</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49998</th>\n",
              "      <td>crazy</td>\n",
              "      <td>दीवाना</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49999</th>\n",
              "      <td>screencast</td>\n",
              "      <td>स्क्रीनकास्ट</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>50000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              english                                              hindi\n",
              "0      give your application an accessibility workout    अपने अनुप्रयोग को पहुंचनीयता व्यायाम का लाभ दें\n",
              "1                   accerciser accessibility explorer                    एक्सेर्साइसर पहुंचनीयता अन्वेषक\n",
              "2      the default plugin layout for the bottom panel               निचले पटल के लिए डिफोल्ट प्लगइन खाका\n",
              "3         the default plugin layout for the top panel                ऊपरी पटल के लिए डिफोल्ट प्लगइन खाका\n",
              "4      a list of plugins that are disabled by default  उन प्लगइनों की सूची जिन्हें डिफोल्ट रूप से निष...\n",
              "...                                               ...                                                ...\n",
              "49995                                      audio test                                         ऑडियो जाँच\n",
              "49996                                          silent                                                मूक\n",
              "49997                                      video test                                        वीडियो जाँच\n",
              "49998                                           crazy                                             दीवाना\n",
              "49999                                      screencast                                       स्क्रीनकास्ट\n",
              "\n",
              "[50000 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHRKCLf6eJhN"
      },
      "source": [
        "test_eng_text = load_doc('english.test.txt')\n",
        "test_hindi_text = load_doc('hindi.test.txt')\n",
        "test_pair = to_pairs(test_eng_text, test_hindi_text)\n",
        "for pair in test_pair:\n",
        "    english_lang.addSentence(pair[0])\n",
        "    hindi_lang.addSentence(pair[1])\n",
        "\n",
        "test_df = pd.DataFrame(test_pair)\n",
        "test_df.columns = [\"english\", \"hindi\"]\n",
        "\n",
        "list_len = []\n",
        "for l in test_df.english:\n",
        "    list_len.append(len(l.split(' ')))\n",
        "max_len_eng = np.max(list_len)\n",
        "\n",
        "list_len = []\n",
        "for l in test_df.hindi:\n",
        "    list_len.append(len(l.split(' ')))\n",
        "max_len_hindi = np.max(list_len)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "_fgWliWGdykJ",
        "outputId": "e656fe65-1a1e-4af8-b3f3-a0447892e33a"
      },
      "source": [
        "test_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>english</th>\n",
              "      <th>hindi</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>a black box in your car</td>\n",
              "      <td>आपकी कार में ब्लैक बॉक्स</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>as americas road planners struggle to find the...</td>\n",
              "      <td>जबकि अमेरिका के सड़क योजनाकार ध्वस्त होते हुए ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>the devices which track every mile a motorist ...</td>\n",
              "      <td>यह डिवाइस जो मोटरचालक द्वारा वाहन चलाए गए प्रत...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>the usually dull arena of highway planning has...</td>\n",
              "      <td>आम तौर पर हाईवे नियोजन जैसा उबाऊ काम भी अचानक ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>libertarians have joined environmental groups ...</td>\n",
              "      <td>आपने द्वारा ड्राइव किए गए मील तथा संभवतः ड्राइ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2502</th>\n",
              "      <td>it is noteworthy that both nita and isha are p...</td>\n",
              "      <td>गौरतलब है कि नीता और ईशा दोनों ही प्रोफेशनल क्...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2503</th>\n",
              "      <td>vips have been invited to this royal party</td>\n",
              "      <td>इस शाही पार्टी के लिए वीवीआईपी लोगों को भी आमं...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2504</th>\n",
              "      <td>these include the jodhpur royal family and uma...</td>\n",
              "      <td>इनमें जोधपुर राजपरिवार और उम्मैद भवन के मालिक ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2505</th>\n",
              "      <td>ln mittal sachin tendulkar and bollywood actor...</td>\n",
              "      <td>एलएनमित्तल सचिन तेंदुलकर फिल्म अभिनेता अनिल कप...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2506</th>\n",
              "      <td>chartered planes have been booked to ferry the...</td>\n",
              "      <td>मेहमानों को लानेले जाने के लिए चार्टर्ड विमानो...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2507 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                english                                              hindi\n",
              "0                               a black box in your car                           आपकी कार में ब्लैक बॉक्स\n",
              "1     as americas road planners struggle to find the...  जबकि अमेरिका के सड़क योजनाकार ध्वस्त होते हुए ...\n",
              "2     the devices which track every mile a motorist ...  यह डिवाइस जो मोटरचालक द्वारा वाहन चलाए गए प्रत...\n",
              "3     the usually dull arena of highway planning has...  आम तौर पर हाईवे नियोजन जैसा उबाऊ काम भी अचानक ...\n",
              "4     libertarians have joined environmental groups ...  आपने द्वारा ड्राइव किए गए मील तथा संभवतः ड्राइ...\n",
              "...                                                 ...                                                ...\n",
              "2502  it is noteworthy that both nita and isha are p...  गौरतलब है कि नीता और ईशा दोनों ही प्रोफेशनल क्...\n",
              "2503         vips have been invited to this royal party  इस शाही पार्टी के लिए वीवीआईपी लोगों को भी आमं...\n",
              "2504  these include the jodhpur royal family and uma...  इनमें जोधपुर राजपरिवार और उम्मैद भवन के मालिक ...\n",
              "2505  ln mittal sachin tendulkar and bollywood actor...  एलएनमित्तल सचिन तेंदुलकर फिल्म अभिनेता अनिल कप...\n",
              "2506  chartered planes have been booked to ferry the...  मेहमानों को लानेले जाने के लिए चार्टर्ड विमानो...\n",
              "\n",
              "[2507 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-NWoOh0e6di",
        "outputId": "1bf34f0f-7dd9-4072-9f50-0c929ccb972c"
      },
      "source": [
        "# Get Glove Vector\n",
        "!wget http://nlp.stanford.edu/data/glove.840B.300d.zip\n",
        "!unzip glove.840B.300d.zip\n",
        "\n",
        "# using 300-dim Glove word embeddings\n",
        "\n",
        "glove_embeddings = {}\n",
        "f = open('glove.840B.300d.txt')\n",
        "for line in f:\n",
        "    values = line.split(' ')\n",
        "    word = values[0] ## The first entry is the word\n",
        "    embedding = np.asarray(values[1:], dtype='float32') ## These are the vecotrs representing the embedding for the word\n",
        "    glove_embeddings[word] = embedding \n",
        "f.close()\n",
        "\n",
        "# initializing unknown token (in case a word is not found, we'll assign it an unknown token)\n",
        "UNK = np.random.random(300)\n",
        "SOS = np.random.random(300)\n",
        "EOS = np.random.random(300)\n",
        "\n",
        "glove_embeddings['sos'] = SOS\n",
        "glove_embeddings['eos'] = EOS\n",
        "\n",
        "print('GloVe data loaded')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-17 15:50:05--  http://nlp.stanford.edu/data/glove.840B.300d.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.840B.300d.zip [following]\n",
            "--2021-04-17 15:50:05--  https://nlp.stanford.edu/data/glove.840B.300d.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.840B.300d.zip [following]\n",
            "--2021-04-17 15:50:06--  http://downloads.cs.stanford.edu/nlp/data/glove.840B.300d.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2176768927 (2.0G) [application/zip]\n",
            "Saving to: ‘glove.840B.300d.zip’\n",
            "\n",
            "glove.840B.300d.zip 100%[===================>]   2.03G  4.87MB/s    in 6m 54s  \n",
            "\n",
            "2021-04-17 15:57:00 (5.02 MB/s) - ‘glove.840B.300d.zip’ saved [2176768927/2176768927]\n",
            "\n",
            "Archive:  glove.840B.300d.zip\n",
            "  inflating: glove.840B.300d.txt     \n",
            "GloVe data loaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLDwWzyPfleE"
      },
      "source": [
        "embedding_size = 300\n",
        "eng_vocab_size = len(english_lang.word2index)+1\n",
        "embedding_matrix_eng = np.zeros((eng_vocab_size,embedding_size))\n",
        "for word,i in sorted( english_lang.word2index.items(), key=lambda x: x[1]):\n",
        "    embedding_value = glove_embeddings.get(word, UNK)\n",
        "    embedding_matrix_eng[i] = embedding_value\n",
        "\n",
        "hindi_vocab_size = len(hindi_lang.word2index)+1\n",
        "embedding_matrix_hindi = np.zeros(( hindi_vocab_size,embedding_size))\n",
        "for word,i in sorted( hindi_lang.word2index.items(), key=lambda x: x[1]):\n",
        "    embedding_value = glove_embeddings.get(word, UNK)\n",
        "    embedding_matrix_hindi[i] = embedding_value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sULBUR6ydzSh"
      },
      "source": [
        "\n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, embedding_size, hidden_size,n_layers=1, bidirectional=True, batch_size=1):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.bidirectional = bidirectional\n",
        "        self.hidden_size = int(hidden_size/2)\n",
        "        self.hidden_dim = self.hidden_size\n",
        "        self.num_layers = n_layers\n",
        "        self.batch_size = batch_size\n",
        "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(embedding_matrix_eng), freeze=True)\n",
        "        self.gru = nn.GRU(embedding_size, self.hidden_size, n_layers*2, bidirectional=bidirectional)\n",
        "        self.linear = nn.Linear(hidden_size * 2, 1)\n",
        "\n",
        "    def forward(self, input, hidden=None):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output, hidden = self.gru(embedded, hidden)\n",
        "        fwd_final = hidden[0:hidden.size(0):2]\n",
        "        bwd_final = hidden[1:hidden.size(0):2]\n",
        "        final = torch.cat([fwd_final, bwd_final], dim=2)\n",
        "        return output, final\n",
        "\n",
        "    def initHidden(self):\n",
        "        directions = 2 if self.bidirectional else 1\n",
        "        return torch.zeros(\n",
        "            self.num_layers*2,\n",
        "            self.batch_size,\n",
        "            self.hidden_size*2,\n",
        "            device=device\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwT13-rohQ5z"
      },
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.2, max_length=hindi_lang.n_words,n_layers=1):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p/5\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(embedding_matrix_hindi), freeze=True)\n",
        "        # self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.attn1 = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size, n_layers*2)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        intmd_torch = torch.cat((embedded[0],hidden[0]),1)\n",
        "        attn_out= self.attn1(intmd_torch)\n",
        "        attn_weights = F.softmax(attn_out)\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "       \n",
        "        output = torch.cat((embedded, attn_applied), 0)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "        output = F.relu(output)\n",
        "        output = torch.squeeze(output,0)\n",
        "        dec_hidden = self.initHidden()\n",
        "        output1, dec_hidden = self.gru(output, hidden)\n",
        "        output2 = F.log_softmax(self.out(output1[0]), dim=1)\n",
        "        return output2, dec_hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1*2, 1, self.hidden_size, device=device)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuE5C0pJignX"
      },
      "source": [
        "\n",
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(english_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(hindi_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXdbPelFkmwQ"
      },
      "source": [
        "\n",
        "teacher_forcing_ratio = 1\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=hindi_lang.n_words):\n",
        "    \n",
        "    encoder_hidden = encoder.initHidden()\n",
        "    \n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "    \n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_dim*2, device=device)\n",
        "    \n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(input_tensor[ei])\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "    \n",
        "    decoder_hidden = encoder_hidden\n",
        "    \n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWJNeagfkupE"
      },
      "source": [
        "\n",
        "\n",
        "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "    \n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    \n",
        "    random.shuffle(train_pair)\n",
        "    \n",
        "    criterion = nn.NLLLoss()\n",
        "    \n",
        "    for iter in range(0, len(train_pair)*2 + 1):\n",
        "        training_pairs = tensorsFromPair(train_pair[(iter)%len(train_pair)])\n",
        "        training_pair = training_pairs\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "        \n",
        "        loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%.4f' % (print_loss_avg))\n",
        "\n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YsPHQq2k2lN"
      },
      "source": [
        "\n",
        "\n",
        "def evaluate(encoder, decoder, sentence, max_length=hindi_lang.n_words):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(english_lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_dim*2, device=device)\n",
        "        \n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei])\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "        \n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "        decoder_attentions = torch.zeros(max_length, max_length)\n",
        "        \n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_attentions[di] = decoder_attention.data\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(hindi_lang.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words, decoder_attentions[:di + 1]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0FaLgCjk8jG"
      },
      "source": [
        "\n",
        "\n",
        "def evaluateRandomly(encoder, decoder, n=20):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(test_pair)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cNG424VlAsT"
      },
      "source": [
        "\n",
        "embed_dim = 300\n",
        "hidden_size = embed_dim\n",
        "hidden_dim = embed_dim\n",
        "n_layers = 1\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "encoder1 = EncoderRNN(english_lang.n_words, embed_dim, hidden_dim, n_layers).to(device)\n",
        "attn_decoder1 = AttnDecoderRNN(embed_dim, hindi_lang.n_words, n_layers).to(device)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CEgNIOXElC4l",
        "outputId": "5bfb3a1b-38f6-4079-9b9a-22fc22ad0670"
      },
      "source": [
        "trainIters(encoder1, attn_decoder1, 100, print_every=5000)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:23: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.0019\n",
            "5.7541\n",
            "5.4143\n",
            "5.3271\n",
            "5.2587\n",
            "5.2438\n",
            "5.2121\n",
            "5.2044\n",
            "5.1514\n",
            "5.1453\n",
            "5.1415\n",
            "5.1106\n",
            "5.1342\n",
            "5.1191\n",
            "5.0908\n",
            "5.1046\n",
            "5.0892\n",
            "5.0964\n",
            "5.0580\n",
            "5.0581\n",
            "5.0610\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSJdv2wileHR"
      },
      "source": [
        "\n",
        "actual, predicted = list(), list()\n",
        "    \n",
        "for i in range(len(test_pair)):\n",
        "    pair = test_pair[i]\n",
        "    output_words, attentions = evaluate(encoder1, attn_decoder1, pair[0])\n",
        "    output_sentence = ' '.join(output_words)\n",
        "    \n",
        "    hindi_list = pair[1].split()\n",
        "    predicted_sen = output_sentence.split()\n",
        "    if (hindi_list[-1] == \"<EOS>\"):\n",
        "        hindi_list = hindi_list[:-1]\n",
        "    if (hindi_list[0] == \"<SOS>\"):\n",
        "        hindi_list.pop(0)\n",
        "    if (predicted_sen[-1] == \"<EOS>\"):\n",
        "        predicted_sen = predicted_sen[:-1]\n",
        "    actual.append(hindi_list)\n",
        "    predicted.append(predicted_sen)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHfLbQ9siGI-",
        "outputId": "ddd6946a-810a-4e60-9740-f13c04d29249"
      },
      "source": [
        "print(\"Bleu Score: \", corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Bleu Score:  0.0532591\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-O-KWX8_iNWk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}